import { WebSite } from "./website";
import { MetricsApi } from "./metrics-api";
import { Construct } from "constructs";

import * as cdk from "aws-cdk-lib";
import * as s3 from "aws-cdk-lib/aws-s3";
import * as iam from "aws-cdk-lib/aws-iam";
import * as ddb from "aws-cdk-lib/aws-dynamodb";
import * as event from "aws-cdk-lib/aws-events";
import * as lambda from "aws-cdk-lib/aws-lambda";
import * as eventTarget from "aws-cdk-lib/aws-events-targets";
import * as lambda_event from "aws-cdk-lib/aws-lambda-event-sources";

export class IronpondStack extends cdk.Stack {
  constructor(scope: Construct, id: string, props?: cdk.StackProps) {
    super(scope, id, props);

    //table for site metrics
    const table = new ddb.Table(this, "MetricsTable", {
      partitionKey: { name: "query_id", type: ddb.AttributeType.STRING },
      sortKey: { name: "query_date", type: ddb.AttributeType.NUMBER },
    });

    const aggregateTable = new ddb.Table(this, "CountryTable", {
      partitionKey: { name: "aggregate", type: ddb.AttributeType.STRING },
    });

    //bucket for storing cloudfront logs. expires after 3 days
    const logBucket = new s3.Bucket(this, "LogBucket", {
      accessControl: s3.BucketAccessControl.BUCKET_OWNER_FULL_CONTROL,
      publicReadAccess: false,
      lifecycleRules: [
        {
          expiration: cdk.Duration.days(3),
        },
      ],
    });

    //instantiate the website class with the logging bucket
    const website = new WebSite(this, "IronPond", {
      logBucket: logBucket,
    });

    new MetricsApi(this, "MetricsApi", {
      aggregateTable: aggregateTable,
      metricsTable: table,
      certificate: website.certificate,
    });

    //a lambda layer build from a physical folder to clean the data queried by athena
    const pandasLayer = new lambda.LayerVersion(this, "PandasLayer", {
      code: lambda.Code.fromAsset("layer"),
      compatibleRuntimes: [lambda.Runtime.PYTHON_3_8],
    });

    //lambda which will, on creation of files in s3
    //organize data generated by an athena query, then store to dynamodb
    const athenaTrigger = new lambda.Function(this, "TriggerLambda", {
      runtime: lambda.Runtime.PYTHON_3_8,
      code: lambda.Code.fromAsset("lambda"),
      handler: "athena_trigger.handler",
      layers: [pandasLayer],
      environment: {
        QUERY_TABLE: table.tableName,
        COUNTRY_TABLE: aggregateTable.tableName,
      },
    });

    //run the lambda when files are create in s3
    const s3PutEventSource = new lambda_event.S3EventSource(logBucket, {
      events: [s3.EventType.OBJECT_CREATED_PUT],
      filters: [{ prefix: "results/", suffix: ".csv" }],
    });

    athenaTrigger.addEventSource(s3PutEventSource);

    //lambda which will run a stored athena query on schedule
    const queryFn = new lambda.Function(this, "QueryLambda", {
      runtime: lambda.Runtime.PYTHON_3_8,
      code: lambda.Code.fromAsset("lambda"),
      handler: "query.handler",
      environment: { BUCKET: logBucket.bucketName },
    });

    //granting neccessary permissions to lambda for querying
    queryFn.role?.attachInlinePolicy(
      new iam.Policy(this, "userpool-policy", {
        statements: [
          new iam.PolicyStatement({
            actions: [
              "athena:RunQuery",
              "athena:StartQueryExecution",
              "athena:GetNamedQuery",
              "glue:GetTable",
            ],
            resources: ["*"],
          }),
        ],
      })
    );

    //event schedule for lambda to run athena query
    new event.Rule(this, "my-lambda-rule", {
      description: "Description of the rule",
      targets: [new eventTarget.LambdaFunction(queryFn)],
      schedule: event.Schedule.cron({ minute: "0", hour: "1" }),
    });

    //granting the lambdas permission to write, read from s3 and write to dynamodb
    logBucket.grantReadWrite(queryFn);
    logBucket.grantReadWrite(athenaTrigger);
    table.grantReadWriteData(athenaTrigger);
    aggregateTable.grantReadWriteData(athenaTrigger);
  }
}
